import json
import random
import os
import requests
import base64
from blog_crawling import crawl_blog
from tistory import crawl_tistory
from enter_crawling import crawl_entertainment
from fashion_crawing import crawl_fashion
from news_crawling import crawl_news
from spots_crawling import crawl_sports

GITHUB_IMAGE_FOLDER = "images/"
GITHUB_REPO_URL = "https://raw.githubusercontent.com/ric-grown/news-bot/main/"  # GitHub Repo Raw URL

# ğŸ”¹ 1ï¸âƒ£ ê¸°ì¡´ ì´ë¯¸ì§€ ì‚­ì œ (í´ë” ì´ˆê¸°í™”)
if os.path.exists(GITHUB_IMAGE_FOLDER):
    for file in os.listdir(GITHUB_IMAGE_FOLDER):
        file_path = os.path.join(GITHUB_IMAGE_FOLDER, file)
        os.remove(file_path)
else:
    os.makedirs(GITHUB_IMAGE_FOLDER)

# ğŸ”¹ 1ï¸âƒ£ ì´ë¯¸ì§€ URLì„ Base64ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_image_to_base64(image_url):
    try:
        response = requests.get(image_url, timeout=5)  # ì´ë¯¸ì§€ ìš”ì²­
        if response.status_code == 200:
            return f"data:image/jpeg;base64,{base64.b64encode(response.content).decode('utf-8')}"
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {image_url}, ì˜¤ë¥˜: {e}")
    return image_url  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ìœ ì§€

# ğŸ”¹ 2ï¸âƒ£ JSON ì €ì¥ í•¨ìˆ˜
def save_json(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ & ì €ì¥ í•¨ìˆ˜
def download_and_save_image(image_url, filename):
    try:
        response = requests.get(image_url, timeout=5)
        if response.status_code == 200:
            with open(os.path.join(GITHUB_IMAGE_FOLDER, filename), "wb") as f:
                f.write(response.content)
            return f"{GITHUB_REPO_URL}{GITHUB_IMAGE_FOLDER}{filename}"
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {image_url}, ì˜¤ë¥˜: {e}")
    return None

# âœ… í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ & ì´ë¯¸ì§€ ì €ì¥
def process_crawled_data(crawled_data):
    updated_data = []
    for i, item in enumerate(crawled_data):
        if "image" in item:
            filename = f"{category}_{i}.jpg"
            image_url = download_and_save_image(item["image"], filename)
            if image_url:
                item["image"] = image_url
        updated_data.append(item)
    return updated_data

# ğŸ”¹ 3ï¸âƒ£ ë°ì´í„° í¬ë¡¤ë§ ë° Base64 ë³€í™˜ ì ìš©
data_sources = {
    "blog_tistory": [],
    "entertainment": [],
    "fashion": [],
    "news": [],
    "sports": []
}

# ğŸ”¹ ë¸”ë¡œê·¸ & í‹°ìŠ¤í† ë¦¬ í¬ë¡¤ë§
blog_data = crawl_blog()
tistory_data = crawl_tistory()
data_sources["blog_tistory"].extend(blog_data + tistory_data)
data_sources["blog_tistory"] = process_crawled_data(data_sources["blog_tistory"])
save_json("blog_tistory.json", data_sources["blog_tistory"])

# ğŸ”¹ ì—”í„°í…Œì¸ë¨¼íŠ¸ í¬ë¡¤ë§
entertainment_data = crawl_entertainment()
data_sources["entertainment"].extend(entertainment_data)
save_json("entertainment.json", data_sources["entertainment"])

# ğŸ”¹ íŒ¨ì…˜ í¬ë¡¤ë§
fashion_data = crawl_fashion()
data_sources["fashion"].extend(fashion_data)
save_json("fashion.json", data_sources["fashion"])

# ğŸ”¹ ë‰´ìŠ¤ í¬ë¡¤ë§
news_data = crawl_news()
data_sources["news"].extend(news_data)
save_json("news.json", data_sources["news"])

# ğŸ”¹ ìŠ¤í¬ì¸  í¬ë¡¤ë§
sports_data = crawl_sports()
data_sources["sports"].extend(sports_data)
data_sources["sports"] = process_crawled_data(data_sources["sports"])
save_json("sports.json", data_sources["sports"])

# ğŸ”¥ 6ï¸âƒ£ ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì³ì„œ ëœë¤ 50ê°œ ì¶”ì¶œ â†’ `hotissue.json` ì €ì¥
all_data = (
    data_sources["blog_tistory"] +
    data_sources["entertainment"] +
    data_sources["fashion"] +
    data_sources["news"] +
    data_sources["sports"]
)

hot_issues = random.sample(all_data, min(50, len(all_data)))
save_json("hotissue.json", hot_issues)

print("âœ… ëª¨ë“  í¬ë¡¤ë§ ë° ì´ë¯¸ì§€ ë³€í™˜ ì‘ì—… ì™„ë£Œ! JSON íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
