import json
import random
import os
import requests
import base64
from blog_crawling import crawl_blog
from tistory import crawl_tistory
from enter_crawling import crawl_entertainment
from fashion_crawing import crawl_fashion
from news_crawling import crawl_news
from spots_crawling import crawl_sports

# ğŸ”¹ 1ï¸âƒ£ ì´ë¯¸ì§€ URLì„ Base64ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def convert_image_to_base64(image_url):
    try:
        response = requests.get(image_url, timeout=5)  # ì´ë¯¸ì§€ ìš”ì²­
        if response.status_code == 200:
            return f"data:image/jpeg;base64,{base64.b64encode(response.content).decode('utf-8')}"
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {image_url}, ì˜¤ë¥˜: {e}")
    return image_url  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ìœ ì§€

# ğŸ”¹ 2ï¸âƒ£ JSON ì €ì¥ í•¨ìˆ˜
def save_json(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# ğŸ”¹ 3ï¸âƒ£ ë°ì´í„° í¬ë¡¤ë§ ë° Base64 ë³€í™˜ ì ìš©
data_sources = {
    "blog_tistory": [],
    "entertainment": [],
    "fashion": [],
    "news": [],
    "sports": []
}

# ğŸ”¹ ë¸”ë¡œê·¸ & í‹°ìŠ¤í† ë¦¬ í¬ë¡¤ë§
blog_data = crawl_blog()
tistory_data = crawl_tistory()
for item in blog_data + tistory_data:
    if "image" in item:
        item["image"] = convert_image_to_base64(item["image"])
data_sources["blog_tistory"].extend(blog_data + tistory_data)
save_json("blog_tistory.json", data_sources["blog_tistory"])

# ğŸ”¹ ì—”í„°í…Œì¸ë¨¼íŠ¸ í¬ë¡¤ë§
entertainment_data = crawl_entertainment()
with open("entertainment.json", "w", encoding="utf-8") as f:
    json.dump(data_sources["entertainment"], f, ensure_ascii=False, indent=4)

# ğŸ”¹ íŒ¨ì…˜ í¬ë¡¤ë§
fashion_data = crawl_fashion()
with open("fashion.json", "w", encoding="utf-8") as f:
    json.dump(data_sources["fashion"], f, ensure_ascii=False, indent=4)

# ğŸ”¹ ë‰´ìŠ¤ í¬ë¡¤ë§
news_data = crawl_news()
with open("news.json", "w", encoding="utf-8") as f:
    json.dump(data_sources["news"], f, ensure_ascii=False, indent=4)

# ğŸ”¹ ìŠ¤í¬ì¸  í¬ë¡¤ë§
sports_data = crawl_sports()
for item in sports_data:
    if "image" in item:
        item["image"] = convert_image_to_base64(item["image"])
data_sources["sports"].extend(sports_data)
save_json("sports.json", data_sources["sports"])

# ğŸ”¥ 6ï¸âƒ£ ëª¨ë“  ë°ì´í„°ë¥¼ í•©ì³ì„œ ëœë¤ 50ê°œ ì¶”ì¶œ â†’ `hotissue.json` ì €ì¥
all_data = (
    data_sources["blog_tistory"] +
    data_sources["entertainment"] +
    data_sources["fashion"] +
    data_sources["news"] +
    data_sources["sports"]
)

hot_issues = random.sample(all_data, min(50, len(all_data)))
save_json("hotissue.json", hot_issues)

print("âœ… ëª¨ë“  í¬ë¡¤ë§ ë° ì´ë¯¸ì§€ ë³€í™˜ ì‘ì—… ì™„ë£Œ! JSON íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
