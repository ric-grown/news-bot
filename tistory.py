from requests_html import HTMLSession
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

# í¬ë¡¤ë§í•  ë„¤ì´ë²„ ë¸”ë¡œê·¸ í˜ì´ì§€ ëª©ë¡
naver_blog_urls = [
    "https://www.tistory.com/?category=travel",
    "https://www.tistory.com/?category=living",
    "https://www.tistory.com/?category=family",
    "https://www.tistory.com/?category=career",
    "https://www.tistory.com/?category=news",
    "https://www.tistory.com/?category=book",
    "https://www.tistory.com/?category=entertainment",
    "https://www.tistory.com/?category=hobby"
]

def crawl_tistory():
    # ì„¸ì…˜ ì‹œì‘
    session = HTMLSession()

    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    all_items = []

    # ê° í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰
    for page_num, url in enumerate(naver_blog_urls, start=1):
        print(f"ğŸ” í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ (í˜ì´ì§€ {page_num}) í¬ë¡¤ë§ ì¤‘...")

        try:
            response = session.get(url)
    
            # JavaScript ë Œë”ë§ ì‹¤í–‰ (3ì´ˆ ëŒ€ê¸°)
            response.html.render(sleep=5, timeout=30)
    
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.html.html, "html.parser")
    
            # ë¸”ë¡œê·¸ ê¸€ì´ í¬í•¨ëœ div ì°¾ê¸°
            article_list = soup.find("div", class_="list_tistory_top")
            
            if article_list:
                articles = article_list.find_all("div", class_="item_group")
                for article in articles:
                        a_tag = article.find("a", class_="link_cont zoom_cont zoom_sm")
                        if a_tag:
                            # ë¸”ë¡œê·¸ ê¸€ ë§í¬ ì°¾ê¸° (desc ë‚´ë¶€ a íƒœê·¸ì˜ ng-href)
                            href = urljoin(url, a_tag["href"]) if a_tag and a_tag.has_attr("href") else None
                            
                            # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì°¾ê¸°
                            img_tag = a_tag.find("img")  # a íƒœê·¸ ë‚´ë¶€ì˜ img íƒœê·¸ ì°¾ê¸°
                            img_src = urljoin(url, img_tag["src"]) if img_tag and img_tag.has_attr("src") else None
                            img_src = img_src.replace("C3x2", "C318x168")
                            img_src = img_src.replace("C2x1", "C160x103")
    
                        # ì œëª© íƒœê·¸ ì°¾ê¸° (desc ë‚´ë¶€ì˜ strong íƒœê·¸)
                        title_tag = article.find("strong")
                        title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
    
                        if href and img_src:
                            all_items.append({"title": title, "link": href, "image": img_src})
        except Exception as e:
            print(f"ğŸš¨ í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        time.sleep(1)
    
    print(f"âœ… ì´ {len(all_items)}ê°œì˜ ë¸”ë¡œê·¸ ê¸€ì„ í¬ë¡¤ë§ ì™„ë£Œ!")
    return all_items

# JSON íŒŒì¼ë¡œ ì €ì¥
#json_file = save_json(all_items, "blog_posts.json")
#print(f"ğŸ“ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_file}")

